"""
Deterministic Research Workflow Endpoint

Part of Workflow-First Architecture (Phase 1):
- Explicit parameters (no TP orchestration)
- Direct specialist invocation
- Full context loading (WorkBundle pattern)
- Auditable execution tracking

Strategic Goal: Prove specialist agents work reliably before adding
TP intelligence layer (Phase 2)
"""

from typing import Optional
from uuid import UUID
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel

from app.utils.jwt import verify_jwt
from app.utils.supabase_client import supabase_admin_client as supabase
from agents_sdk.research_agent_sdk import ResearchAgentSDK
from agents_sdk.work_bundle import WorkBundle
from shared.session import AgentSession
import logging

router = APIRouter(prefix="/work/research", tags=["workflows"])
logger = logging.getLogger(__name__)


class ResearchWorkflowRequest(BaseModel):
    """Deterministic research workflow parameters."""
    basket_id: str
    task_description: str
    research_scope: Optional[str] = "general"  # general, competitor, market, technical
    depth: Optional[str] = "standard"  # quick, standard, deep
    output_format: Optional[str] = "markdown"  # markdown, json, structured
    priority: Optional[int] = 5


class ResearchWorkflowResponse(BaseModel):
    """Research workflow execution result."""
    work_request_id: str
    work_ticket_id: str
    agent_session_id: str
    status: str  # pending, running, completed, failed
    outputs: list[dict]  # work_outputs generated by agent
    execution_time_ms: Optional[int]
    message: str


@router.post("/execute", response_model=ResearchWorkflowResponse)
async def execute_research_workflow(
    request: ResearchWorkflowRequest,
    user: dict = Depends(verify_jwt)
):
    """
    Execute deterministic research workflow.

    Flow:
    1. Validate permissions (workspace, basket, trial limits)
    2. Load context (WorkBundle: blocks + assets + config)
    3. Create work_request + work_ticket (tracking)
    4. Execute ResearchAgentSDK with context
    5. Return structured outputs

    Args:
        request: Research workflow parameters
        user: Authenticated user from JWT

    Returns:
        Research workflow execution result with outputs

    Raises:
        401: Authentication failed
        403: Permission denied or trial limit exceeded
        404: Basket not found
        500: Execution error
    """
    user_id = user.get("sub") or user.get("user_id")
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid user token")

    logger.info(
        f"[RESEARCH WORKFLOW] Starting: user={user_id}, basket={request.basket_id}"
    )

    try:
        # Step 1: Validate basket access and get workspace
        basket_response = supabase.table("baskets").select(
            "id, workspace_id, name"
        ).eq("id", request.basket_id).single().execute()

        if not basket_response.data:
            raise HTTPException(status_code=404, detail="Basket not found")

        basket = basket_response.data
        workspace_id = basket["workspace_id"]

        # TODO: Add workspace permission check (verify user has access)
        # TODO: Add trial limit check (work_requests count)

        # Step 2: Get or create research agent session (persistent per basket)
        research_session = await AgentSession.get_or_create(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            agent_type="research",
            user_id=user_id,
        )

        logger.info(
            f"[RESEARCH WORKFLOW] Agent session: {research_session.id}"
        )

        # Step 3: Create work_request (for trial tracking & billing)
        work_request_data = {
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "requested_by_user_id": user_id,
            "request_type": "research_workflow",
            "task_intent": request.task_description,
            "parameters": {
                "research_scope": request.research_scope,
                "depth": request.depth,
                "output_format": request.output_format,
            },
            "priority": "normal",
        }
        work_request_response = supabase.table("work_requests").insert(
            work_request_data
        ).execute()
        work_request_id = work_request_response.data[0]["id"]

        # Step 4: Create work_ticket (execution tracking)
        work_ticket_data = {
            "work_request_id": work_request_id,
            "agent_session_id": research_session.id,
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "agent_type": "research",
            "status": "pending",
            "metadata": {
                "workflow": "deterministic_research",
                "task_description": request.task_description,
                "research_scope": request.research_scope,
                "depth": request.depth,
            },
        }
        work_ticket_response = supabase.table("work_tickets").insert(
            work_ticket_data
        ).execute()
        work_ticket_id = work_ticket_response.data[0]["id"]

        logger.info(
            f"[RESEARCH WORKFLOW] Created: work_request={work_request_id}, "
            f"work_ticket={work_ticket_id}"
        )

        # Step 5: Create WorkBundle (metadata only) + SubstrateQueryAdapter (on-demand)
        logger.info(f"[RESEARCH WORKFLOW] Creating context for basket {request.basket_id}")

        # 5a. Load prior work outputs (recent research to avoid duplication)
        prior_outputs_response = supabase.table("work_outputs").select(
            "id, title, output_type, body, confidence, created_at"
        ).eq("basket_id", request.basket_id).eq(
            "agent_type", "research"
        ).eq("status", "approved").order(
            "created_at", ascending=False
        ).limit(50).execute()

        prior_work_outputs = prior_outputs_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(prior_work_outputs)} prior outputs")

        # 5b. Load reference assets (documents, screenshots, etc.)
        assets_response = supabase.table("documents").select(
            "id, title, document_type, metadata"
        ).eq("basket_id", request.basket_id).execute()

        reference_assets = assets_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(reference_assets)} reference assets")

        # 5c. Create WorkBundle (metadata only - NO substrate_blocks)
        # Agents query substrate on-demand via SubstrateQueryAdapter
        context_bundle = WorkBundle(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            user_id=user_id,
            task=request.task_description,
            agent_type="research",
            priority="medium",
            reference_assets=reference_assets,
            agent_config={},  # Use defaults
        )

        # 5d. Create SubstrateQueryAdapter for on-demand substrate access
        from adapters.substrate_adapter import SubstrateQueryAdapter
        substrate_adapter = SubstrateQueryAdapter(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            agent_type="research",
            work_ticket_id=work_ticket_id,
        )

        logger.info(
            f"[RESEARCH WORKFLOW] Context created: "
            f"{len(reference_assets)} assets, {len(prior_work_outputs)} prior outputs, "
            f"SubstrateQueryAdapter for on-demand queries"
        )

        # Step 6: Update work_ticket status to running
        supabase.table("work_tickets").update({
            "status": "running",
            "started_at": "now()",
        }).eq("id", work_ticket_id).execute()

        # Step 7: Execute ResearchAgentSDK with context
        logger.info(f"[RESEARCH WORKFLOW] Executing ResearchAgentSDK")

        # Initialize ResearchAgentSDK with bundle + substrate adapter
        research_sdk = ResearchAgentSDK(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            work_ticket_id=work_ticket_id,
            session=research_session,
            substrate=substrate_adapter,  # On-demand substrate queries
            bundle=context_bundle,  # WorkBundle (metadata only)
        )

        # Build enhanced prompt with prior work context
        enhanced_task = request.task_description
        if prior_work_outputs:
            enhanced_task += "\n\n**Prior Research** (avoid duplication):\n"
            for output in prior_work_outputs[:5]:  # Show last 5
                enhanced_task += f"- {output['title']} ({output['output_type']})\n"

        # Execute deep dive research
        import time
        start_time = time.time()

        result = await research_sdk.deep_dive(
            topic=enhanced_task,
            claude_session_id=research_session.claude_session_id,
        )

        execution_time_ms = int((time.time() - start_time) * 1000)

        # Get final TodoWrite state from task_streaming
        from app.work.task_streaming import TASK_UPDATES
        final_todos = TASK_UPDATES.get(work_ticket_id, [])

        # Step 8: Update work_ticket status to completed
        supabase.table("work_tickets").update({
            "status": "completed",
            "completed_at": "now()",
            "metadata": {
                "execution_time_ms": execution_time_ms,
                "output_count": result["output_count"],
                "claude_session_id": result.get("claude_session_id"),
                "final_todos": final_todos,  # Store historical TodoWrite data
            },
        }).eq("id", work_ticket_id).execute()

        # Clean up TodoWrite data
        TASK_UPDATES.pop(work_ticket_id, None)

        logger.info(
            f"[RESEARCH WORKFLOW] Execution complete: {result['output_count']} outputs "
            f"in {execution_time_ms}ms"
        )

        return ResearchWorkflowResponse(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            agent_session_id=research_session.id,
            status="completed",
            outputs=result["work_outputs"],
            execution_time_ms=execution_time_ms,
            message=f"Research complete: {result['output_count']} outputs generated",
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"[RESEARCH WORKFLOW] Failed: {e}")

        # Update work_ticket to failed status if it exists
        if 'work_ticket_id' in locals():
            try:
                supabase.table("work_tickets").update({
                    "status": "failed",
                    "completed_at": "now()",
                    "metadata": {
                        "error": str(e),
                        "error_type": type(e).__name__,
                    },
                }).eq("id", work_ticket_id).execute()
            except Exception as update_error:
                logger.error(f"Failed to update work_ticket status: {update_error}")

        raise HTTPException(
            status_code=500,
            detail=f"Research workflow execution failed: {str(e)}"
        )
