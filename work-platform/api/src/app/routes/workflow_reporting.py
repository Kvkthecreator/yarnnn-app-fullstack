"""
Deterministic Reporting Workflow Endpoint

Part of Workflow-First Architecture:
- Explicit parameters (no TP orchestration)
- Direct specialist invocation via ReportingAgent
- Full context loading (research findings, data)
- Auditable execution tracking
- Optional recipe-driven execution (parameterized templates)

Architecture (Post-SDK Removal):
- Uses ReportingAgent (Skills API for PPTX, XLSX, DOCX, PDF)
- First-principled context: research-oriented, document generation
- Tool execution via Skills API code_execution
"""

from typing import Optional, Dict, Any, List
from uuid import UUID
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from datetime import datetime, timezone

from app.utils.jwt import verify_jwt
from app.utils.supabase_client import supabase_admin_client as supabase
from agents.reporting_agent import ReportingAgent, create_reporting_agent
from services.recipe_loader import RecipeLoader, RecipeValidationError
import logging
import time

router = APIRouter(prefix="/work/reporting", tags=["workflows"])
logger = logging.getLogger(__name__)


class ReportingWorkflowRequest(BaseModel):
    """Deterministic reporting workflow parameters."""
    basket_id: str
    task_description: str
    output_format: Optional[str] = "pptx"  # pptx, xlsx, docx, pdf
    document_title: Optional[str] = None
    template_style: Optional[str] = "professional"  # professional, minimal, branded
    include_data: Optional[Dict[str, Any]] = None  # Structured data for charts/tables
    priority: Optional[int] = 5

    # Recipe integration (optional)
    recipe_id: Optional[str] = None  # Recipe UUID or slug
    recipe_parameters: Optional[Dict[str, Any]] = None  # User-customized parameters
    reference_asset_ids: Optional[List[str]] = None  # User-uploaded assets

    # Async execution mode
    async_execution: Optional[bool] = False  # If True, return ticket_id immediately


class ReportingWorkflowResponse(BaseModel):
    """Reporting workflow execution result."""
    work_request_id: str
    work_ticket_id: str
    status: str  # pending, running, completed, failed
    outputs: List[dict]  # work_outputs generated by agent
    execution_time_ms: Optional[int]
    message: str
    recipe_used: Optional[str] = None  # Recipe slug if recipe-driven
    token_usage: Optional[Dict[str, int]] = None  # Token usage stats


@router.post("/execute", response_model=ReportingWorkflowResponse)
async def execute_reporting_workflow(
    request: ReportingWorkflowRequest,
    user: dict = Depends(verify_jwt)
):
    """
    Execute deterministic reporting workflow.

    Uses ReportingAgent with Claude Skills API for document generation.
    Supports PPTX, XLSX, DOCX, PDF output formats.

    Flow:
    1. Validate permissions (workspace, basket)
    2. Create work_request + work_ticket (tracking)
    3. Execute ReportingAgent with Skills API
    4. Return structured outputs

    Args:
        request: Reporting workflow parameters
        user: Authenticated user from JWT

    Returns:
        Reporting workflow execution result with outputs

    Raises:
        401: Authentication failed
        403: Permission denied
        404: Basket or recipe not found
        400: Invalid recipe parameters or unsupported format
        500: Execution error
    """
    user_id = user.get("sub") or user.get("user_id")
    user_token = user.get("token")  # JWT for substrate-API auth
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid user token")

    # Validate output format
    supported_formats = ["pptx", "xlsx", "docx", "pdf"]
    if request.output_format and request.output_format.lower() not in supported_formats:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported output format: {request.output_format}. Supported: {supported_formats}"
        )

    logger.info(
        f"[REPORTING WORKFLOW] Starting: user={user_id}, basket={request.basket_id}, "
        f"format={request.output_format}, recipe={request.recipe_id}"
    )

    try:
        # Step 1: Validate basket access and get workspace
        basket_response = supabase.table("baskets").select(
            "id, workspace_id, name"
        ).eq("id", request.basket_id).single().execute()

        if not basket_response.data:
            raise HTTPException(status_code=404, detail="Basket not found")

        basket = basket_response.data
        workspace_id = basket["workspace_id"]

        # Step 2: Recipe-driven execution (if recipe_id provided)
        recipe = None
        execution_context = None
        validated_params = None

        if request.recipe_id:
            logger.info(f"[REPORTING WORKFLOW] Loading recipe: {request.recipe_id}")

            loader = RecipeLoader()

            # Load recipe by ID or slug
            try:
                recipe = await loader.load_recipe(recipe_id=request.recipe_id)
            except Exception:
                recipe = await loader.load_recipe(slug=request.recipe_id)

            logger.info(f"[REPORTING WORKFLOW] Loaded recipe: {recipe.name} (v{recipe.version})")

            # Validate parameters
            try:
                validated_params = loader.validate_parameters(
                    recipe=recipe,
                    user_parameters=request.recipe_parameters or {}
                )
                logger.info(f"[REPORTING WORKFLOW] Validated parameters: {validated_params}")
            except RecipeValidationError as e:
                raise HTTPException(
                    status_code=400,
                    detail=f"Recipe parameter validation failed: {str(e)}"
                )

            # Generate execution context
            execution_context = loader.generate_execution_context(
                recipe=recipe,
                validated_parameters=validated_params
            )

        # Step 3: Create work_request (for tracking & billing)
        work_request_data = {
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "requested_by_user_id": user_id,
            "request_type": f"recipe_{recipe.slug}" if recipe else "reporting_workflow",
            "task_intent": recipe.name if recipe else request.task_description,
            "parameters": {
                "output_format": request.output_format,
                "document_title": request.document_title,
                "template_style": request.template_style,
                "recipe_used": recipe.slug if recipe else None,
                "recipe_parameters": validated_params if recipe else None,
            },
            "priority": "normal",
        }

        if recipe:
            work_request_data["recipe_id"] = recipe.id
            work_request_data["recipe_parameters"] = validated_params
            work_request_data["reference_asset_ids"] = request.reference_asset_ids or []

        work_request_response = supabase.table("work_requests").insert(
            work_request_data
        ).execute()
        work_request_id = work_request_response.data[0]["id"]

        # Step 4: Create work_ticket (execution tracking)
        work_ticket_data = {
            "work_request_id": work_request_id,
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "agent_type": "reporting",
            "status": "pending",
            "metadata": {
                "workflow": "recipe_reporting" if recipe else "deterministic_reporting",
                "task_description": request.task_description,
                "output_format": request.output_format,
                "template_style": request.template_style,
                "recipe_slug": recipe.slug if recipe else None,
                "recipe_id": recipe.id if recipe else None,
                "execution_mode": "skills_api",  # Mark as using Skills API
            },
        }
        work_ticket_response = supabase.table("work_tickets").insert(
            work_ticket_data
        ).execute()
        work_ticket_id = work_ticket_response.data[0]["id"]

        logger.info(
            f"[REPORTING WORKFLOW] Created: work_request={work_request_id}, "
            f"work_ticket={work_ticket_id}"
        )

        # ASYNC MODE: Return immediately, execute in background
        if request.async_execution:
            logger.info(f"[REPORTING WORKFLOW] Async mode: returning ticket_id immediately")

            import threading
            import asyncio as bg_asyncio

            # Capture closure variables
            _basket_id = request.basket_id
            _workspace_id = workspace_id
            _user_id = user_id
            _user_token = user_token
            _work_request_id = work_request_id
            _work_ticket_id = work_ticket_id
            _task_description = request.task_description
            _output_format = request.output_format
            _document_title = request.document_title
            _template_style = request.template_style
            _include_data = request.include_data
            _recipe = recipe
            _execution_context = execution_context
            _validated_params = validated_params

            def execute_in_background():
                logger.info(f"[REPORTING WORKFLOW] Background thread started for ticket {_work_ticket_id}")

                # Create a new event loop for this thread
                loop = bg_asyncio.new_event_loop()
                bg_asyncio.set_event_loop(loop)
                logger.info(f"[REPORTING WORKFLOW] Event loop created")

                try:
                    from app.utils.supabase_client import supabase_admin_client as bg_supabase
                    from agents.reporting_agent import ReportingAgent
                    from app.work.task_streaming import emit_task_update
                    logger.info(f"[REPORTING WORKFLOW] Imports successful")

                    # Update status to running
                    bg_supabase.table("work_tickets").update({
                        "status": "running",
                        "started_at": datetime.now(timezone.utc).isoformat(),
                    }).eq("id", _work_ticket_id).execute()
                    logger.info(f"[REPORTING WORKFLOW] Ticket status updated to running")

                    # Emit initial task update
                    emit_task_update(_work_ticket_id, {
                        "type": "task_started",
                        "status": "in_progress",
                        "current_step": "Initializing document generation",
                        "activeForm": f"Setting up {_output_format.upper()} generation",
                    })
                    logger.info(f"[REPORTING WORKFLOW] Emitted task_started")

                    # Build enhanced task if recipe-driven
                    enhanced_task = _task_description
                    if _execution_context:
                        task_breakdown = _execution_context.get("task_breakdown", [])
                        deliverable_intent = _execution_context.get("deliverable_intent", {})
                        enhanced_task = f"""**Recipe: {_recipe.name}**

**Deliverable Intent:**
- Purpose: {deliverable_intent.get('purpose', 'Create document')}
- Audience: {deliverable_intent.get('audience', 'Stakeholders')}
- Outcome: {deliverable_intent.get('outcome', 'Professional document')}

**Task:** {_task_description}

**Task Breakdown:**
{chr(10).join([f"- {step}" for step in task_breakdown])}

**Parameters:**
- Format: {_output_format}
- Slide Count: {_validated_params.get('slide_count', 5) if _validated_params else 5}
- Focus Area: {_validated_params.get('focus_area', 'None specified') if _validated_params else 'None specified'}

{_execution_context.get('system_prompt_additions', '')}
"""

                    # Emit context loading update
                    emit_task_update(_work_ticket_id, {
                        "type": "task_update",
                        "status": "in_progress",
                        "current_step": "Loading context",
                        "activeForm": "Loading research findings and data",
                    })

                    # Create executor and run
                    executor = ReportingAgent(
                        basket_id=_basket_id,
                        workspace_id=_workspace_id,
                        work_ticket_id=_work_ticket_id,
                        user_id=_user_id,
                        user_jwt=_user_token,
                    )

                    # Emit generation update
                    emit_task_update(_work_ticket_id, {
                        "type": "task_update",
                        "status": "in_progress",
                        "current_step": "Generating document",
                        "activeForm": f"Creating {_output_format.upper()} with Skills API",
                    })

                    start_time = time.time()
                    result = loop.run_until_complete(executor.execute(
                        task=enhanced_task,
                        output_format=_output_format,
                        document_title=_document_title,
                        include_data=_include_data,
                        template_style=_template_style,
                    ))
                    execution_time_ms = int((time.time() - start_time) * 1000)

                    # Emit completion update
                    emit_task_update(_work_ticket_id, {
                        "type": "task_completed",
                        "status": "completed",
                        "current_step": "Document generation complete",
                        "activeForm": f"Generated {len(result.work_outputs)} outputs",
                        "output_count": len(result.work_outputs),
                    })

                    # Get final todos before cleanup
                    from app.work.task_streaming import get_final_todos, cleanup_task_updates
                    final_todos = get_final_todos(_work_ticket_id)

                    # Update to completed
                    existing_ticket = bg_supabase.table("work_tickets").select("metadata").eq("id", _work_ticket_id).single().execute()
                    existing_metadata = existing_ticket.data.get("metadata", {}) if existing_ticket.data else {}

                    updated_metadata = {
                        **existing_metadata,
                        "execution_time_ms": execution_time_ms,
                        "output_count": len(result.work_outputs),
                        "final_todos": final_todos,  # Store task history for UI display
                        "token_usage": {
                            "input_tokens": result.input_tokens,
                            "output_tokens": result.output_tokens,
                            "cache_read_tokens": result.cache_read_tokens,
                        },
                    }

                    bg_supabase.table("work_tickets").update({
                        "status": "completed",
                        "completed_at": datetime.now(timezone.utc).isoformat(),
                        "metadata": updated_metadata,
                    }).eq("id", _work_ticket_id).execute()

                    # Cleanup in-memory task updates
                    cleanup_task_updates(_work_ticket_id)

                    logger.info(f"[REPORTING WORKFLOW] Background complete: {len(result.work_outputs)} outputs, {len(final_todos)} todos stored")

                except Exception as e:
                    logger.exception(f"[REPORTING WORKFLOW] Background execution failed: {e}")
                    import traceback
                    logger.error(f"[REPORTING WORKFLOW] Traceback: {traceback.format_exc()}")
                    try:
                        from app.utils.supabase_client import supabase_admin_client as err_supabase
                        from app.work.task_streaming import emit_task_update as emit_err

                        # Emit error update
                        emit_err(_work_ticket_id, {
                            "type": "task_failed",
                            "status": "failed",
                            "current_step": "Execution failed",
                            "activeForm": str(e)[:100],
                            "error": str(e),
                        })

                        err_supabase.table("work_tickets").update({
                            "status": "failed",
                            "completed_at": datetime.now(timezone.utc).isoformat(),
                            "error_message": str(e),
                        }).eq("id", _work_ticket_id).execute()
                        logger.info(f"[REPORTING WORKFLOW] Updated ticket to failed status")
                    except Exception as update_err:
                        logger.error(f"[REPORTING WORKFLOW] Failed to update ticket: {update_err}")
                finally:
                    # Clean up the event loop
                    logger.info(f"[REPORTING WORKFLOW] Closing event loop")
                    try:
                        loop.close()
                    except Exception as close_err:
                        logger.warning(f"[REPORTING WORKFLOW] Error closing loop: {close_err}")

            thread = threading.Thread(target=execute_in_background, daemon=True)
            thread.start()

            return ReportingWorkflowResponse(
                work_request_id=work_request_id,
                work_ticket_id=work_ticket_id,
                status="running",
                outputs=[],
                execution_time_ms=None,
                message="Document generation started in background - track progress via work_ticket status",
                recipe_used=recipe.slug if recipe else None,
            )

        # SYNC MODE: Execute and wait for result
        logger.info(f"[REPORTING WORKFLOW] Sync mode: executing document generation")

        # Update status to running
        supabase.table("work_tickets").update({
            "status": "running",
            "started_at": datetime.now(timezone.utc).isoformat(),
        }).eq("id", work_ticket_id).execute()

        # Build enhanced task if recipe-driven
        enhanced_task = request.task_description
        if execution_context:
            task_breakdown = execution_context.get("task_breakdown", [])
            deliverable_intent = execution_context.get("deliverable_intent", {})
            enhanced_task = f"""**Recipe: {recipe.name}**

**Deliverable Intent:**
- Purpose: {deliverable_intent.get('purpose', 'Create document')}
- Audience: {deliverable_intent.get('audience', 'Stakeholders')}
- Outcome: {deliverable_intent.get('outcome', 'Professional document')}

**Task:** {request.task_description}

**Task Breakdown:**
{chr(10).join([f"- {step}" for step in task_breakdown])}

**Parameters:**
- Format: {request.output_format}
- Slide Count: {validated_params.get('slide_count', 5)}
- Focus Area: {validated_params.get('focus_area', 'None specified')}

{execution_context.get('system_prompt_additions', '')}
"""

        # Create executor and run
        executor = create_reporting_agent(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            work_ticket_id=work_ticket_id,
            user_id=user_id,
            user_jwt=user_token,
        )

        start_time = time.time()
        result = await executor.execute(
            task=enhanced_task,
            output_format=request.output_format,
            document_title=request.document_title,
            include_data=request.include_data,
            template_style=request.template_style,
        )
        execution_time_ms = int((time.time() - start_time) * 1000)

        # Update to completed
        supabase.table("work_tickets").update({
            "status": "completed",
            "completed_at": datetime.now(timezone.utc).isoformat(),
            "metadata": {
                "workflow": "recipe_reporting" if recipe else "deterministic_reporting",
                "execution_time_ms": execution_time_ms,
                "output_count": len(result.work_outputs),
                "recipe_slug": recipe.slug if recipe else None,
                "token_usage": {
                    "input_tokens": result.input_tokens,
                    "output_tokens": result.output_tokens,
                    "cache_read_tokens": result.cache_read_tokens,
                },
            },
        }).eq("id", work_ticket_id).execute()

        logger.info(
            f"[REPORTING WORKFLOW] Complete: {len(result.work_outputs)} outputs "
            f"in {execution_time_ms}ms, tokens={result.input_tokens}+{result.output_tokens}"
        )

        return ReportingWorkflowResponse(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            status="completed",
            outputs=result.work_outputs,
            execution_time_ms=execution_time_ms,
            message=f"Document complete: {len(result.work_outputs)} outputs generated",
            recipe_used=recipe.slug if recipe else None,
            token_usage={
                "input_tokens": result.input_tokens,
                "output_tokens": result.output_tokens,
                "cache_read_tokens": result.cache_read_tokens,
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"[REPORTING WORKFLOW] Failed: {e}")

        if 'work_ticket_id' in locals():
            try:
                supabase.table("work_tickets").update({
                    "status": "failed",
                    "completed_at": datetime.now(timezone.utc).isoformat(),
                    "metadata": {
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "recipe_slug": recipe.slug if recipe and 'recipe' in locals() else None,
                    },
                }).eq("id", work_ticket_id).execute()
            except Exception as update_error:
                logger.error(f"Failed to update work_ticket status: {update_error}")

        raise HTTPException(
            status_code=500,
            detail=f"Reporting workflow execution failed: {str(e)}"
        )


@router.get("/status")
async def reporting_workflow_status():
    """
    Get reporting workflow status.

    Returns:
        Workflow status information
    """
    return {
        "status": "active",
        "message": "Reporting workflow is available with Skills API support",
        "supported_formats": ["pptx", "xlsx", "docx", "pdf"],
        "features": {
            "skills_api": True,
            "recipe_driven": True,
            "async_execution": True,
        },
        "recipes": [
            "executive-summary-deck",
        ],
    }
